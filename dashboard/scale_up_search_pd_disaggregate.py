from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
import sys
from typing import Any

import pandas as pd
import streamlit as st

# Ensure the repository root is importable even when the script is executed directly.
_REPO_ROOT = Path(__file__).resolve().parents[1]
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

from dashboard._paths import ensure_repo_root_on_path

ensure_repo_root_on_path()

from dashboard.features import (
    ChipSpec,
    ChunkedPrefill,
    KvCacheTraffic,
    plot_metric_vs_batch,
    run_scaleup_search_fixedN,
)
try:  # pragma: no cover - allow running as a script
    from dashboard.services.llm_calcs import (
        ModelProfile,
        concurrency_adjusted_times,
        effective_compute_tflops,
        kv_cache_memory_traffic,
        prefill_decode_time_breakdown,
    )
except ImportError:  # pragma: no cover - executed when imported as package module
    from .services.llm_calcs import (
        ModelProfile,
        concurrency_adjusted_times,
        effective_compute_tflops,
        kv_cache_memory_traffic,
        prefill_decode_time_breakdown,
    )

from dashboard.app_context import DashboardActions, DashboardState, bootstrap


@dataclass
class _SearchConfig:
    chip: ChipSpec
    sla_ttft_ms: float
    sla_tpot_ms: float
    avg_input: int
    avg_output: int
    seq_len_kv: int
    dtype_bytes: int
    chunked_prefill: ChunkedPrefill
    kv_cache_hit: float
    decode_priority: float
    concurrency: int
    alpha_conc: float
    spec_speedup: float
    causal_mask: bool
    attn_impl: str


@dataclass(frozen=True)
class _ConcurrencySummary:
    """Helper container for UI-friendly concurrency metrics."""

    ttft_ms: float
    tpot_ms: float
    throughput_tps: float
    n_eq: float
    overlap_effective: float


def render(state: DashboardState, actions: DashboardActions) -> None:
    st = state.st
    session_state = state.session_state
    model = state.model

    st.header(
        "ğŸ§® Scale-up Search Â· PDåˆ†ç¦» Â· Dense/MoE/GQA/MLA/Linear Attention æ¨¡å‹è‡ªé€‚åº”ç‰ˆ"
    )

    cfg = getattr(model, "cfg", getattr(model, "raw_cfg", {})) or {}

    def _cfg_get(cfg_obj: Any, keys, default=None):
        for k in keys:
            if isinstance(cfg_obj, dict) and k in cfg_obj:
                return cfg_obj[k]
            v = getattr(cfg_obj, k, None)
            if v is not None:
                return v
            if hasattr(cfg_obj, "model"):
                m = getattr(cfg_obj, "model")
                if isinstance(m, dict) and k in m:
                    return m[k]
                if hasattr(m, k):
                    return getattr(m, k)
        return default

    def parse_model_spec(cfg_obj: Any):
        H_val = int(_cfg_get(cfg_obj, ["num_attention_heads", "n_heads", "num_heads"], 0) or 0)
        D_val = int(_cfg_get(cfg_obj, ["hidden_size", "d_model", "model_dim"], 0) or 0)
        L_val = int(_cfg_get(cfg_obj, ["num_hidden_layers", "n_layers", "layers"], 0) or 0)
        head_dim_val = int(_cfg_get(cfg_obj, ["head_dim", "qk_head_dim", "kv_channels"], 0) or 0)
        inter_sz = int(_cfg_get(cfg_obj, ["intermediate_size", "ffn_hidden_size"], 0) or 0)
        ffn_mult_val = float(_cfg_get(cfg_obj, ["ffn_mult", "mlp_ratio"], 0.0) or 0.0)
        if D_val <= 0 and H_val > 0 and head_dim_val > 0:
            D_val = H_val * head_dim_val
        if ffn_mult_val <= 0 and inter_sz > 0 and D_val > 0:
            ffn_mult_val = inter_sz / D_val
        if head_dim_val <= 0 and D_val > 0 and H_val > 0:
            head_dim_val = D_val // H_val
        return H_val, D_val, L_val, head_dim_val, ffn_mult_val, inter_sz

    H, D, L, head_dim, ffn_mult, _ = parse_model_spec(cfg)
    if H == 0 or D == 0 or L == 0:
        st.warning("âš ï¸ æ— æ³•ä»cfgè§£ææ¨¡å‹å‚æ•°ï¼Œè¯·ç¡®è®¤å·²åŠ è½½å®Œæ•´é…ç½®ã€‚")

    with st.expander("Search å‚æ•°", expanded=True):
        c0, c1, c2 = st.columns(3)
        N_cards = c0.number_input("Total GPUs N (fixed)", 1, 65536, 64, 1, key="search_N")
        sla_ttft_ms = c1.number_input("SLA: TTFT (ms)", 0.0, value=120.0, step=1.0, key="sla_ttft")
        sla_tpot_ms = c2.number_input("SLA: TPOT (ms/token)", 0.0, value=2.0, step=0.1, key="sla_tpot")

        c3, c4, c5 = st.columns(3)
        avg_input = c3.number_input("å¹³å‡è¾“å…¥ tokens (avg_input)", 1, 32768, 2048, step=128, key="avg_in_tokens")
        avg_output = c4.number_input("å¹³å‡è¾“å‡º tokens (avg_output)", 1, 32768, 256, step=16, key="avg_out_tokens")
        seq_len_kv = c5.number_input("Decode KV é•¿åº¦ (L_kv)", 128, 131072, 4096, step=128, key="seq_len_kv")

    with st.expander("ç¡¬ä»¶å‚æ•°", expanded=True):
        c5, c6, c7 = st.columns(3)
        tflops = c5.number_input("èŠ¯ç‰‡å³°å€¼ç®—åŠ› (TFLOPs)", 10.0, 2000.0, 600.0, step=10.0)
        mfu = c6.slider("æœ‰æ•ˆ MFU", 0.05, 1.0, 0.4, 0.05)
        hbm_bw = c7.number_input("HBM å¸¦å®½ (GB/s)", 100.0, 6000.0, 3000.0, step=100.0)

        c8, c9 = st.columns(2)
        hbm_eff = c8.slider("HBM åˆ©ç”¨ç‡ (æœ‰æ•ˆ)", 0.05, 1.0, 0.6, 0.05)
        clk_GHz = c9.number_input("GPU æ—¶é’Ÿé¢‘ç‡ (GHz)", 0.5, 3.0, 1.8, 0.1)

    with st.expander("Prefill / Decode è°ƒåº¦å‚æ•°", expanded=True):
        c10, c11, c12 = st.columns(3)
        chunked_prefill_value = c10.slider("Chunked Prefill å¼ºåº¦", 0.0, 1.0, 0.5, 0.05)
        decode_priority = c11.slider("Decode ä¼˜å…ˆçº§", 0.0, 1.0, 0.7, 0.05)
        kv_cache_hit = c12.slider("KV Cache å‘½ä¸­ç‡", 0.0, 1.0, 0.9, 0.05)

        c13, c14, _ = st.columns(3)
        causal_mask = c13.checkbox("ä½¿ç”¨ Causal Mask", value=True)
        attn_impl = c14.selectbox("Attention ç±»å‹", ["standard", "GQA", "MLA", "linear"], index=0)
        dtype_bytes = 2

    with st.expander("å¹¶å‘å‚æ•° (Prefill/Decode Overlap ä¿®æ­£)", expanded=True):
        c16, c17, c18 = st.columns(3)
        concurrency = c16.number_input("å®é™…å¹¶å‘åº¦ (N_conc)", 1, 1024, 16, 1)
        alpha_conc = c17.slider("å¹¶å‘å¹³æ»‘ç³»æ•° Î±", 1.0, 3.0, 1.7, 0.1)
        spec_speedup = c18.slider("Speculative è§£ç åŠ é€Ÿ", 1.0, 3.0, 1.3, 0.1)

    do_search = st.button(
        "å¼€å§‹æœç´¢",
        type="primary",
        use_container_width=True,
        key="scale_up_dashboard_pd_disagg_run_search",
    )

    search_cfg = _SearchConfig(
        chip=ChipSpec(float(tflops), float(mfu), float(hbm_bw), float(hbm_bw * 0.3)),
        sla_ttft_ms=float(sla_ttft_ms),
        sla_tpot_ms=float(sla_tpot_ms),
        avg_input=int(avg_input),
        avg_output=int(avg_output),
        seq_len_kv=int(seq_len_kv),
        dtype_bytes=int(dtype_bytes),
        chunked_prefill=ChunkedPrefill(float(chunked_prefill_value), float(decode_priority)),
        kv_cache_hit=float(kv_cache_hit),
        decode_priority=float(decode_priority),
        concurrency=int(concurrency),
        alpha_conc=float(alpha_conc),
        spec_speedup=float(spec_speedup),
        causal_mask=bool(causal_mask),
        attn_impl=str(attn_impl),
    )

    refresh_token_key = "refresh_token_pd_disagg"
    df_key = "df_search_pd_disagg"

    if do_search:
        session_state[refresh_token_key] = int(session_state.get(refresh_token_key, 0)) + 1
        df_search = run_scaleup_search_fixedN(
            cfg=cfg,
            N=int(N_cards),
            seq_len=search_cfg.avg_input,
            kv_len_decode=search_cfg.seq_len_kv,
            dtype_bytes=search_cfg.dtype_bytes,
            kv_dtype_bytes=search_cfg.dtype_bytes,
            top_k_override=None,
            chip=search_cfg.chip,
            overlap=0.0,
            sla_ttft_ms=search_cfg.sla_ttft_ms,
            sla_tpot_ms=search_cfg.sla_tpot_ms,
            hbm_capacity_GB=80.0,
            hbm_reserve_ratio=0.1,
            include_scores=True,
            grad_accum=int(session_state.get("grad_accum", 1)),
            refresh_token=int(session_state[refresh_token_key]),
        )
        session_state[df_key] = df_search

    df_search = session_state.get(df_key, pd.DataFrame())

    if df_search.empty:
        st.info("ç‚¹å‡» `Run search` ç”Ÿæˆé…ç½®å¯¹æ¯”è¡¨ã€‚")
        return

    df = df_search.copy()
    df["H"], df["D"], df["L"] = H, D, L
    df["head_dim"] = head_dim
    df["ffn_mult"] = ffn_mult
    df["avg_input"] = search_cfg.avg_input
    df["avg_output"] = search_cfg.avg_output

    profile = ModelProfile(
        model,
        weight_dtype_bytes=search_cfg.dtype_bytes,
        kv_dtype_bytes=search_cfg.dtype_bytes,
        seq_len_in=search_cfg.avg_input,
        kv_len_in=search_cfg.seq_len_kv,
        include_scores=True,
        top_k=None,
    )
    comp_df = profile.component_dataframe()

    st.subheader("æ–¹æ¡ˆå¯¹æ¯”è¡¨")
    st.dataframe(df, use_container_width=True)

    st.subheader("ç®—åŠ›/å¸¦å®½ åˆ©ç”¨ç‡")
    best_idx = None
    if "throughput_seq_per_s" in df.columns and not df["throughput_seq_per_s"].isna().all():
        best_idx = df["throughput_seq_per_s"].astype(float).idxmax()
    elif "TTFT_ms" in df.columns:
        best_idx = df["TTFT_ms"].astype(float).idxmin()
    if best_idx is None:
        best_row = df.iloc[0]
    else:
        best_row = df.loc[best_idx]

    tp_eff = int(best_row.get("TP", 1))

    memory = kv_cache_memory_traffic(
        profile,
        input_tokens=int(search_cfg.avg_input),
        kv_len_decode=int(search_cfg.seq_len_kv),
        kv_cache_hit=float(search_cfg.kv_cache_hit),
        tp=int(tp_eff),
    )

    eff_compute = effective_compute_tflops(float(tflops), float(mfu))
    hbm_eff_adj = search_cfg.chunked_prefill.adjust_hbm_efficiency(float(hbm_eff))

    times = prefill_decode_time_breakdown(
        flops_prefill=float(profile.prefill_totals.get("total", 0.0)),
        flops_decode=float(profile.decode_totals.get("total", 0.0)),
        effective_tflops=float(eff_compute),
        memory=memory,
        hbm_bw_GBs=float(hbm_bw),
        hbm_eff=float(hbm_eff_adj),
    )

    conc_adjusted = concurrency_adjusted_times(
        times,
        concurrency=float(search_cfg.concurrency),
        alpha=float(search_cfg.alpha_conc),
    )

    spec_speedup = max(1.0, float(search_cfg.spec_speedup))
    tpot_spec_ms = float(conc_adjusted.tpot_eff_ms) / spec_speedup
    throughput_tps = (
        float(search_cfg.concurrency) * 1000.0 / tpot_spec_ms if tpot_spec_ms > 0 else 0.0
    )

    conc_times = _ConcurrencySummary(
        ttft_ms=float(conc_adjusted.ttft_eff_ms),
        tpot_ms=tpot_spec_ms,
        throughput_tps=throughput_tps,
        n_eq=float(conc_adjusted.n_eq),
        overlap_effective=float(conc_adjusted.overlap_effective),
    )

    c19, c20 = st.columns(2)
    c19.metric("Effective TFLOPs", f"{eff_compute:.1f}")
    c20.metric("Concurrency-adjusted TTFT", f"{conc_times.ttft_ms:.1f} ms")

    st.subheader("TTFT vs. Batch per GPU")
    fig_ttft = plot_metric_vs_batch(df, metric="ttft_ms")
    st.plotly_chart(fig_ttft, use_container_width=True)

    st.subheader("TPOT vs. Batch per GPU")
    fig_tpot = plot_metric_vs_batch(df, metric="tpot_ms")
    st.plotly_chart(fig_tpot, use_container_width=True)

    st.subheader("Prefill/Decode Breakdown")
    breakdown_df = pd.DataFrame(
        {
            "Stage": ["Prefill", "Decode"],
            "Compute (ms)": [times.t_comp_prefill_ms, times.t_comp_decode_ms],
            "HBM (ms)": [times.t_hbm_prefill_ms, times.t_hbm_decode_ms],
            "Theoretical (ms)": [times.ttft_theory_ms, times.tpot_theory_ms],
            "After concurrency (ms)": [conc_adjusted.ttft_eff_ms, conc_adjusted.tpot_eff_ms],
            "After speculative (ms)": [conc_times.ttft_ms, conc_times.tpot_ms],
        }
    )
    st.dataframe(breakdown_df, use_container_width=True)

    st.subheader("KV Cache Traffic")
    kv_traffic = KvCacheTraffic(
        df=df, seq_len_kv=search_cfg.seq_len_kv, dtype_bytes=search_cfg.dtype_bytes
    )
    st.plotly_chart(kv_traffic.plot(), use_container_width=True)

    st.subheader("å¹¶å‘ä¿®æ­£ç»“æœ")
    st.markdown(
        f"TTFT: {conc_times.ttft_ms:.2f} ms Â· TPOT: {conc_times.tpot_ms:.3f} ms/token Â· Throughput: {conc_times.throughput_tps:.2f} tok/s"
    )


def main() -> None:
    help_markdown = (
        "**å¯ä»¥åšä»€ä¹ˆ**\n\n"
        "- ä»¥ç»™å®šæ¨¡å‹/ç¡¬ä»¶é…ç½®ä¸ºåŸºç¡€ï¼Œä¼°ç®—åœ¨ä¸åŒå¹¶å‘ä¸åˆ†ç‰‡ç­–ç•¥ä¸‹çš„ TTFTã€TPOT ä¸ååã€‚\n"
        "- æ”¯æŒ MoE / GQA / MLA ç­‰å¤šç§æ³¨æ„åŠ›å˜ä½“ï¼Œå¹¶å¯å¯¹ PD åˆ†ç¦»ç­–ç•¥è¿›è¡Œå¿«é€Ÿæœç´¢ã€‚\n\n"
        "**ä¸»è¦å¯è°ƒå‚æ•°**\n\n"
        "- **æ¨¡å‹æ¨æ–­**ï¼šè‡ªåŠ¨è§£æ cfg ä¸­çš„ Hã€Dã€L ç­‰å‚æ•°ï¼Œä¹Ÿå¯é€šè¿‡ä¾§è¾¹æ è°ƒæ•´ dtype ä¸ KV é•¿åº¦ã€‚\n"
        "- **Search é…ç½®**ï¼šåŒ…å« SLA ç›®æ ‡ã€å¹³å‡ prompt/output é•¿åº¦ã€å¹¶å‘åº¦ã€KV cache å‘½ä¸­ç‡ã€spec decode speedup ç­‰ã€‚\n"
        "- **Chunked Prefill è®¾ç½®**ï¼šåœ¨ç•Œé¢ä¸­è®¾å®š chunk å¤§å°ã€prefill å¹¶è¡Œåº¦ï¼Œå¹¶å¯¹ PD åˆ†ç¦»æ–¹æ¡ˆè¿›è¡Œè¯„ä¼°ã€‚\n"
        "- **å¹¶å‘ä¿®æ­£**ï¼šè‡ªå®šä¹‰ alpha_concã€decode priorityã€causal maskã€attention å®ç°ï¼Œå½±å“æœ€ç»ˆååä¼°è®¡ã€‚"
    )

    state, actions = bootstrap(
        "Scale-up Search (PD åˆ†ç¦»)",
        header_description="æœç´¢æ»¡è¶³ SLA çš„å¹¶å‘/åˆ†ç‰‡é…ç½®ï¼Œå¹¶ä¼°ç®—ç®—åŠ›ä¸å¸¦å®½éœ€æ±‚ (PD åˆ†ç¦»ç‰ˆ)ã€‚",
        help_title="Scale-up Search (PD åˆ†ç¦») å¸®åŠ©",
        help_markdown=help_markdown,
    )
    render(state, actions)


if __name__ == "__main__":
    main()
